{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 09 - Topic Modeling - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome back! Today, we'll work on a very useful text analysis technique called Topic Modeling. Particularly, we will use its most popular and powerful algorithm: Latent Dirichlet Allocation, or LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you implement your own LDA on STAN, it is important that you understand all concepts well. In this notebook, we will use an \"off-the-shelf\" LDA implementation, from scikit-learn (sklearn), and we'll play a little bit with the Dirichlet distribution. Only after that, you'll do your own LDA in STAN. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by the usual imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "import pystan_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import LDA from sklearn.\n",
    "\n",
    "In order to use it, one needs to convert the documents into a Bag-of-Words representation. The object CountVectorizer does that, so let's import that too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different kind of dataset, for a change. \n",
    "\n",
    "Python has pretty neat packages for data collection, and one that is pretty interesting is the wikipedia package. You can play with that tremendously powerful resource! :-)\n",
    "\n",
    "First, please install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to play with pages that have location labels. We will center our search in Copenhagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=wikipedia.geosearch(55.676098, 12.568337, results=200, radius=10000) # check the Wikipedia Python API package to \n",
    "# see the possibilities!\n",
    "\n",
    "#Or, you could try other kinds of pages... \n",
    "#pp=wikipedia.search('norse gods', results=150)  #uncomment this line if you want to try using the search method (we tested with Norse Gods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be curious about what the variable pp  contains. Plese check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timeline of Copenhagen',\n",
       " 'Copenhagen',\n",
       " 'Rådhuspladsen Station',\n",
       " 'Dragon Fountain, Copenhagen',\n",
       " 'City Hall Square, Copenhagen',\n",
       " '2000 UEFA Cup Final riots',\n",
       " 'Copenhagen Fire of 1728',\n",
       " \"Jens Olsen's World Clock\",\n",
       " 'Industriens Hus',\n",
       " 'Palace Hotel (Copenhagen)',\n",
       " 'Lur Blowers',\n",
       " 'Copenhagen City Hall',\n",
       " 'Copenhagen Municipality',\n",
       " 'Vestergade',\n",
       " 'Grand Theatre (Copenhagen)',\n",
       " 'H. C. Andersens Boulevard',\n",
       " 'National Scala',\n",
       " 'Circus Building, Copenhagen',\n",
       " 'Pantomimeteatret',\n",
       " 'Lavendelstræde',\n",
       " 'Søren Kierkegaard Research Center',\n",
       " 'Danish Cultural Institute',\n",
       " 'Tre Hjorter',\n",
       " 'Copenhagen Waterworks',\n",
       " 'Axeltorv',\n",
       " 'Vester Voldgade',\n",
       " 'Studiestræde',\n",
       " 'Axelborg',\n",
       " 'Socialist Youth Front',\n",
       " 'Tivoli Gardens',\n",
       " 'Eurovision Song Contest 1964',\n",
       " 'Copenhagen Central Fire Station',\n",
       " 'Suhr House',\n",
       " 'Assessor Bachmann House',\n",
       " 'Danish Agency for Culture and Palaces',\n",
       " 'INDEX: Design to Improve Life',\n",
       " 'Danish Design Centre',\n",
       " 'Nimb Hotel',\n",
       " 'Palads Teatret',\n",
       " 'Danish Arts Foundation',\n",
       " 'Dansk Sprognævn',\n",
       " 'Copenhagen Court House',\n",
       " 'Danish Association of Pharmaconomists',\n",
       " 'Gammeltorv',\n",
       " 'Det Harboeske Enkefruekloster',\n",
       " 'Holm House',\n",
       " 'Caritas Well',\n",
       " 'Nytorv',\n",
       " 'Copenhagen City Hall (1728–95)',\n",
       " 'Copenhagen City Hall (1479–1728)',\n",
       " 'Ording House',\n",
       " 'Krak House',\n",
       " 'Jarmers Plads',\n",
       " 'Sankt Peders Stræde',\n",
       " 'Radisson Collection Hotel, Royal Copenhagen',\n",
       " 'Stelling House',\n",
       " 'Frisch House',\n",
       " 'Jens Lauritzen House',\n",
       " 'Vesterport Station',\n",
       " 'University of Copenhagen Faculty of Law',\n",
       " 'Vandkunsten (square)',\n",
       " 'Tivolis Koncertsal',\n",
       " 'Liberty Column, Copenhagen',\n",
       " 'Stormgade',\n",
       " 'LGBT Danmark',\n",
       " 'Dronningens Enghave',\n",
       " 'Dæmonen',\n",
       " 'Holstein Mansion',\n",
       " 'Valkendorfs Kollegium',\n",
       " 'Karel van Mander House',\n",
       " 'Star Flyer (Tivoli Gardens)',\n",
       " 'Bispetorv, Copenhagen',\n",
       " 'Reformation Memorial, Copenhagen',\n",
       " 'Bernstorffsgade',\n",
       " 'Sankt Petri Schule',\n",
       " 'Bispegården, Copenhagen',\n",
       " 'Brolæggerstræde',\n",
       " 'Dantes Plads',\n",
       " 'Dyrkøb',\n",
       " 'Povl Badstuber House',\n",
       " 'Kingittorsuaq Runestone',\n",
       " 'Ny Vestergade',\n",
       " 'Trold, der vejrer kristenblod',\n",
       " 'Copenhagen Central Station',\n",
       " 'Pan Club Copenhagen',\n",
       " 'Nobis Hotel Copenhagen',\n",
       " 'Copenhagen metropolitan area',\n",
       " 'Knabrostræde',\n",
       " 'Realdania',\n",
       " 'Magstræde',\n",
       " 'Latin Quarter, Copenhagen',\n",
       " \"St. Peter's Church, Copenhagen\",\n",
       " 'Operation Carthage',\n",
       " 'Ny Carlsberg Glyptotek',\n",
       " 'Larslejsstræde',\n",
       " 'Imperial Theater, Copenhagen',\n",
       " 'Battle of Copenhagen (1807)',\n",
       " 'Church of Our Lady (Copenhagen)',\n",
       " 'Kunstnerkollegiet',\n",
       " 'Danish Runic Inscription 107',\n",
       " 'Asferg Runestone',\n",
       " 'Schäffer House',\n",
       " 'Frue Plads',\n",
       " 'Ny Vestergade 13',\n",
       " 'Tryggevælde Runestone',\n",
       " 'Sønder Kirkeby Runestone',\n",
       " 'Sørup runestone',\n",
       " 'Christus (statue)',\n",
       " 'Jorcks Passage',\n",
       " 'University of Copenhagen',\n",
       " 'Novo Nordisk Foundation Center for Protein Research',\n",
       " 'Stormbro',\n",
       " 'Strædet',\n",
       " 'Badstuestræde',\n",
       " 'Sankt Petri Passage',\n",
       " 'Ziegler House, Copenhagen',\n",
       " 'The Physicist Hans Christian Ørsted',\n",
       " 'Strøget',\n",
       " 'Ny Kongensgade',\n",
       " 'Ørstedsparken',\n",
       " 'Pæretræet',\n",
       " 'Frederiksholms Kanal 16–18',\n",
       " 'Skindergade',\n",
       " 'Copenhagen University Library',\n",
       " 'Royal Pawn (Denmark)',\n",
       " 'Klosterstræde',\n",
       " 'Bertel Thorvaldsens Plads',\n",
       " 'Nørregade',\n",
       " 'Ny Kongensgade 11',\n",
       " 'Ny Kongensgade 6',\n",
       " 'Vesterbro Pharmacy',\n",
       " 'Frederiksholms Kanal 20',\n",
       " 'Ny Kongensgade 7',\n",
       " 'Gammel Strand 52',\n",
       " 'Copenhagen Central Post Building',\n",
       " 'Gammel Strand 50',\n",
       " 'Store Kannikestræde',\n",
       " 'Klostergården, Copenhagen',\n",
       " 'Barchmann Mansion',\n",
       " 'Kunstforeningen',\n",
       " 'House of the Holy Ghost, Copenhagen',\n",
       " 'Folketeatret, Copenhagen',\n",
       " 'Royal Stables (Denmark)',\n",
       " 'Borchs Kollegium',\n",
       " 'Franciscan Friary, Copenhagen',\n",
       " 'Gråbrødretorv',\n",
       " 'Church of the Holy Ghost, Copenhagen',\n",
       " 'Krystalgade',\n",
       " 'Thorvaldsen Museum',\n",
       " 'Johan Borups Højskole',\n",
       " 'Gammel Strand',\n",
       " 'Amagertorv',\n",
       " 'Gyldenløvesgade',\n",
       " 'Fiolstræde',\n",
       " 'Copenhagen Police Headquarters',\n",
       " 'Zinn House',\n",
       " 'Theatre Museum in the Court Theatre',\n",
       " 'Nørre Voldgade',\n",
       " 'Christiansborg Palace',\n",
       " 'Supreme Court (Denmark)',\n",
       " 'Tycho Brahe Planetarium',\n",
       " \"Elers' Kollegium\",\n",
       " 'Niels Hemmingsens Gade',\n",
       " 'Valkendorfsgade',\n",
       " 'Civiletatens Materialgård',\n",
       " 'Royal Horse Guards Barracks (Copenhagen)',\n",
       " 'Pressens Hus',\n",
       " 'Café Teatret',\n",
       " 'Regensen',\n",
       " 'Ingerslevsgade',\n",
       " 'Folketing',\n",
       " \"N. Zahle's School\",\n",
       " 'Roman Catholic Diocese of Copenhagen',\n",
       " 'Færøsk Pakhus',\n",
       " 'Fæstningens Materialgård',\n",
       " 'Rysensteen Gymnasium',\n",
       " 'Gammel Strand Station',\n",
       " 'Højbro Plads 6',\n",
       " 'Mathias Hansen House',\n",
       " 'Copenhagen Castle',\n",
       " 'Højbro',\n",
       " 'Water Art',\n",
       " 'DGI-byen',\n",
       " 'Rundetaarn',\n",
       " 'Equestrian statue of Absalon',\n",
       " 'Agnete and the Merman',\n",
       " 'The Crystal, Copenhagen',\n",
       " 'University of Copenhagen Faculty of Theology',\n",
       " \"Absalon's Castle\",\n",
       " 'Stork Fountain',\n",
       " 'Israels Plads',\n",
       " 'Købmagergade',\n",
       " 'Abel Cathrines Stiftelse',\n",
       " 'Christiansborg Palace (1st)',\n",
       " 'Christiansborg Palace (2nd)',\n",
       " 'Proviantgården',\n",
       " \"Christian IV's Arsenal\",\n",
       " 'Royal Danish Arsenal Museum',\n",
       " 'National Museum of Photography (Denmark)',\n",
       " 'Højbro Plads']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a simple wrapper, that gets the summary content of each page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#The idea of this wrapper is to get a content summary for each page. This summary will be a \"document\" in your LDA\n",
    "#Sometimes, there is no summary in a wikipedia page, so the call wikipedia.page(title).summary returns an error\n",
    "#\n",
    "#In Python, to make sure your program doesn't just stop, you need to use the \"try ... except\" block\n",
    "\n",
    "def get_wiki_content(title):\n",
    "    try:   \n",
    "        return wikipedia.page(title).summary  \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return 'NA'   #In case there was an error in the call, you'll get 'NA'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all the text, then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A set of texts is called a \"corpus\"\n",
    "corpus=[get_wiki_content(page) for page in pp]  # this may take a while... \n",
    "\n",
    "corpus=list(filter(('NA').__ne__, corpus))  #remove all the 'NA' documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a careful look at the corpus variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create our Bag-of-Words representation (BoW). Here's how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(stop_words='english', max_df=1.0, min_df=0.025) #create a CountVectorizer object. stop_words is a list a of words that are \n",
    "                                                 #irrelevant (for example, at, in, on, are, be, have...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We should add some new stop-words that could make sense in our case... For example, the word \"copenhagen\" will be everywhere and is not informative at all (if the search is centered in Copenhagen, what should the documents be about?...)! Also, there are \"syntactic\" words that relate to the webpages collected (e.g. \"document\", \"com\", \"window\"...). \n",
    "\n",
    "If we don't clean the texts, our topics will become full of noise (just try commenting the code below and see the results).\n",
    "\n",
    "**in fact, after seeing your results, you may want to come back here to extend your stop word list...**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "my_stop_words=text.ENGLISH_STOP_WORDS.union(['copenhagen', 'danish', 'denmark', 'document', 'function', 'id', 'com', 'window', 'true', 'onload', 'app', 'check', 'time', 'adroll', 'parentnode','hereor','getelementsbytagname','script','download', 'search', 'online', 'view','emailprotected', 'emailing', 'calling'])\n",
    "vectorizer=CountVectorizer(stop_words=my_stop_words, max_df=1.0, min_df=0.04) #create a CountVectorizer object. stop_words is a list a of words that are \n",
    "                                                 #irrelevant (for example, at, in, on, are, be, have...)\n",
    "                                                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As many other objects in Sklearn, CountVectorizer is applied with the function fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_bow=vectorizer.fit_transform(corpus)   #creates a BoW representation\n",
    "description_vocabulary = vectorizer.get_feature_names()  #gets the words that correspond to each element of the BoWa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be confusing. Check carefully the content of the variables you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of documents is %d\"%len(corpus))\n",
    "print(\"number of different words is %d\"% len(description_vocabulary))\n",
    "print(\"Hence, shape of token matrix is\", descriptions_bow.get_shape())\n",
    "print(\"Second document text:\\n\", corpus[1])\n",
    "print(\"BoW (after stopwords removed):\\n\", descriptions_bow[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is finally time to run our LDA! We will try with sklearn first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda=LatentDirichletAllocation(n_components=10, learning_method='batch')\n",
    "x=lda.fit_transform(descriptions_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to understand well both objects, x and lda. Check them carefully... for example, check what methods are available in the lda object. And check the dimensionality of x. What does it mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handy code from Sklearn website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it to see your topics. Can you tell the general concept behind each one?\n",
    "\n",
    "Can you propose the concept \"underlying\" each topic (or at least some of them):\n",
    "- Which topics relate to historical buildings?\n",
    "- Which topics relate to transport?\n",
    "- Which topics relate to culture?\n",
    "\n",
    "Do you want to change the parameters (number of topics, values for alpha and beta, stop word list...) and see the changes in the topics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words=12\n",
    "print_top_words(lda, description_vocabulary, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Understanding the dirichlet distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dirichlet distribution is available as numpy.random.dirichlet(alpha, size=None)\n",
    "\n",
    "...so, try it! For example, obtain draws from this distribution using different values of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.dirichlet([.2,.2, .2]))\n",
    "print(np.random.dirichlet([.1,.1, .9]))\n",
    "print(np.random.dirichlet([1,1, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you can, try to visualize it. Remember what we did in the slides. Try to do the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feel free to use the function below, to plot points from a dirichlet distribution, onto a 2D simplex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to plot points in a simplex'''\n",
    "\n",
    "# Based on post from Thomas Boggs (http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/)\n",
    "\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "_corners = np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]])\n",
    "_triangle = tri.Triangulation(_corners[:, 0], _corners[:, 1])\n",
    "\n",
    "def plot_points(X):\n",
    "    '''Plots a set of points in the simplex.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `X` (ndarray): A 2xN array (if in Cartesian coords) or 3xN array\n",
    "                       (if in barycentric coords) of points to plot.\n",
    "    '''\n",
    "    \n",
    "    X = X.dot(_corners)  #This is what converts the original points onto the simplex (it projects on it, through dot product)\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', ms=1)\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 0.75**0.5)\n",
    "    plt.axis('off')\n",
    "    plt.triplot(_triangle, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 10000 points from the dirichlet distribution and plot them using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts=np.random.dirichlet([.1,.5,.5], size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points(pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with different values of $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "alphas = [[0.999] * 3,\n",
    "          [5] * 3,\n",
    "          [2, 5, 15]]\n",
    "for (i, alpha) in enumerate(alphas):\n",
    "    plt.subplot(2, len(alphas), i + 1)\n",
    "    dist = np.random.dirichlet(alpha, size=5000)\n",
    "    title = r'$\\alpha$ = (%.3f, %.3f, %.3f)' % tuple(alpha)\n",
    "    plt.title(title, fontdict={'fontsize': 8})\n",
    "    plot_points(dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Implement your own LDA model in STAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to make your own model in STAN! :-) Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LDA_STAN=\"\"\"\n",
    "data{\n",
    "    int<lower=1> I;\n",
    "    int<lower=1> J[I];  // We mean J=||W_i||\n",
    "    int<lower=2> K;\n",
    "    int<lower=2> C;\n",
    "    vector<lower=0>[K] alpha;\n",
    "    vector<lower=0>[C] beta;\n",
    "    int<lower=2> MAX_J;\n",
    "    int W[I,MAX_J];\n",
    "    }\n",
    "\n",
    "parameters{\n",
    "    simplex[K] theta[I];\n",
    "    simplex[C] phi[K];\n",
    "}\n",
    "\n",
    "model{\n",
    "        \n",
    "    for (k in 1:K)\n",
    "        phi[k]~dirichlet(beta);\n",
    "        \n",
    "    for (i in 1:I){\n",
    "        theta[i]~dirichlet(alpha);\n",
    "        \n",
    "        for (j in 1:J[i]){\n",
    "                real gamma[K];\n",
    "                for (k in 1:K)\n",
    "                    gamma[k]=log(theta[i,k])+log(phi[k,W[i][j]]);\n",
    "                \n",
    "                target+=(log_sum_exp(gamma));\n",
    "           }\n",
    "    \n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preparation can be a bit tricky, so we did it for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyzer=vectorizer.build_analyzer()\n",
    "\n",
    "K=10    #number of topics\n",
    "I=len(corpus)               #number of documents\n",
    "C=len(description_vocabulary)  #number of different words in the dictionary\n",
    "alpha=np.full(K, 1.0/K)     #alpha and\n",
    "beta=np.full(C, 1.0/C)      #beta parameters\n",
    "J=np.zeros(I, dtype='int')   #each document will have a different number of words (we initialize it here, but complete it below)\n",
    "\n",
    "\n",
    "#The list D will contain a list of documents\n",
    "D=[]        \n",
    "for i in range(I):  \n",
    "    valid_words=[w for w in analyzer(corpus[i]) if w in description_vocabulary]\n",
    "    d=[description_vocabulary.index(w)+1 for w in valid_words]  #each document is a sequence of words\n",
    "    if len(d)==0:\n",
    "        continue    #sometimes it happens that a document becomes empty after cleaning it up...\n",
    "    D.append(d)\n",
    "    \n",
    "I=len(D) #if there were empty documents (due to the data cleaning), we removed them, so we need to decrease I\n",
    "    \n",
    "J=[len(d) for d in D]   \n",
    "MAX_J= max(J)  \n",
    "\n",
    "#W is the matrix with all words from all documents, to send STAN\n",
    "W=np.zeros((I, MAX_J), dtype='int')   \n",
    "W=[np.pad(d, (0, MAX_J-len(d)), 'constant', constant_values=0) for d in D]\n",
    "W=np.array(W, dtype='int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the \"data\" dictionary, to send to STAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data={...}\n",
    "data={'I':I, 'J':J, 'K':K, 'C':C, 'alpha':alpha, 'beta':beta, 'MAX_J':MAX_J, 'W':W}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the STAN model now. The VB version will be faster, as usual, and if you're running this in the class, this may be important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create and run Stan model object\n",
    "sm = pystan.StanModel(model_code=LDA_STAN)\n",
    "fit = sm.vb(data=data, iter=10000, algorithm=\"meanfield\", elbo_samples=100, grad_samples=20, seed=42, verbose=True)\n",
    "#fit = sm.sampling(data=data, iter=1000, chains=4, algorithm=\"NUTS\", verbose=True)\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we create a few function to help you inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vb_extract_variable_set(fit, varnames):\n",
    "    full_names=[name for name in fit['sampler_param_names'] if sum([name.startswith(vname) for vname in varnames])>0]\n",
    "    l=[]\n",
    "    for fn in full_names:\n",
    "        l.append(pystan_utils.vb_extract_variable(fit, fn))\n",
    "    \n",
    "    return full_names, l\n",
    "\n",
    "\n",
    "def extract_topic_VB(fit, K, N, phi='phi'):\n",
    "    topic=[]\n",
    "    for n in range(N):\n",
    "        topic.append(pystan_utils.vb_extract_variable(fit, phi+\".\"+str(K+1)+\".\"+str(n+1)))\n",
    "    return np.array(topic)\n",
    "    \n",
    "\n",
    "def print_top_words_VB(model, feature_names, n_top_words, K, phi='phi'):\n",
    "    N=len(feature_names)\n",
    "    for k in range(K):\n",
    "        topic=extract_topic_VB(fit, k, N, phi)\n",
    "        message = \"Topic #%d: \" % k\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "    \n",
    "def print_top_words_STAN(model, feature_names, n_top_words, K):\n",
    "    for k in range(K):\n",
    "        topic=np.mean(model.extract('phi')['phi'][:,k,:], axis=0)\n",
    "        message = \"Topic #%d: \" % k\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please print the top 12 words for your topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words_VB(fit, description_vocabulary, 12, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as just a useful tip, we want to help you save your models after estimating (you don't want to re-run STAN again if you have estimated the model before!). \n",
    "\n",
    "The code below serves to save the STAN model and the \"fit\" variable, which has the results of the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"model_fit.pkl\", \"wb\") as f:\n",
    "    pickle.dump({'model' : sm, 'fit' : fit}, f, protocol=-1)\n",
    "    # or with a list\n",
    "    # pickle.dump([model, fit], f, protocol=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below lets you load it again later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"model_fit.pkl\", \"rb\") as f:\n",
    "    data_dict = pickle.load(f)\n",
    "    # or with a list\n",
    "    # data_list = pickle.load(f)\n",
    "fit = data_dict['fit']\n",
    "# fit = data_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
